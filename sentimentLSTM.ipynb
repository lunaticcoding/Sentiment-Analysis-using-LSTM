{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sentiment Analysis LSTM Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "I tried to rewrite the neural network described in the Towards Datascience article \"Sentiment Analysis using LSTM\" (http://app.n26.com) using the IMDB Review Dataset from http://ai.stanford.edu/~amaas/data/sentiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Version should be 2.0 or higher.\nCurrent version is \"2.0.0-alpha0\".\n"
     ]
    }
   ],
   "source": [
    "print('Version should be 2.0 or higher.\\nCurrent version is \"{}\".'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the data from 'from http://ai.stanford.edu/~amaas/data/sentiment'. This takes some time to run, so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unpacking dataset...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Downloading dataset...')\n",
    "\n",
    "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "urllib.request.urlretrieve(url, 'data.tar.gz')\n",
    "\n",
    "print('Unpacking dataset...')\n",
    "\n",
    "with tarfile.open('data.tar.gz', \"r:gz\") as tar:\n",
    "    tar.extractall()\n",
    "    \n",
    "print('Done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataframes by loading the data from the downloaded file and adding the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataset(train_test):\n",
    "    \"\"\"\n",
    "        Creates a Dataframe with one review column containing the reviews as Strings and a \n",
    "        sentiment column of datatype int with 0 being negative and 1 being positive sentiment.\n",
    "        \n",
    "    :param train_test: Accepts either 'train' or 'test' to create the respective Dataframe. \n",
    "    :return: A Dataframe containing the reviews and the sentiment.\n",
    "    \"\"\"\n",
    "    pos = load_data_as_dataframe(train_test, 'pos')\n",
    "    neg = load_data_as_dataframe(train_test, 'neg')\n",
    "    \n",
    "    df = pd.DataFrame(pos)\n",
    "    df.append(neg, ignore_index=True)\n",
    "    \n",
    "    df['sentiment'] = pd.to_numeric(df['sentiment'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data_as_dataframe(train_test, label):\n",
    "    \"\"\"\n",
    "        This function retrieves the data with a certain label from the directory specified.\n",
    "    \n",
    "    :param train_test: A string being either 'train' or 'test', specifying which directory should be targeted.\n",
    "    :param label: A string being either 'pos' or 'neg' specifying if the positive or the negative reviews should be assembled.\n",
    "    :return: A Dataframe with a review column (dtype=String) and a sentiment column (dtype=Int).\n",
    "    \"\"\"\n",
    "    sentiment = 1 if label == 'pos' else 0\n",
    "    reviews = gather_reviews_from_directory(train_test, label)\n",
    "    reviews = pd.DataFrame({'review': reviews})\n",
    "    reviews['sentiment'] = pd.Series(np.array([sentiment] * len(reviews)), index=reviews.index)\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def gather_reviews_from_directory(train_test, label):\n",
    "    \"\"\"\n",
    "        This function gathers the reviews from the single files and returns them as a Pandas \n",
    "        Series Object.\n",
    "        \n",
    "    :param label: A string being either 'pos' or 'neg' specifying if the positive or the negative reviews should be assembled.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for filepath in os.listdir('aclImdb/' + train_test + '/' + label):\n",
    "        with open(('aclImdb/' + train_test + '/' + label + '/' + filepath), 'r') as file:\n",
    "            review = file.read()\n",
    "            content.append(review)\n",
    "            \n",
    "    return pd.Series(content)\n",
    "\n",
    "\n",
    "train = create_dataset('train')\n",
    "test = create_dataset('test')\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major change to the code in the article is in how words that occur in the training but not in the test set. In the article the mapping is collected from the entire dataset, here it is collected only from the training set. This makes an unknown word token necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_WORD_TOKEN = -1\n",
    "\n",
    "\n",
    "def word_to_int_mapping(df, vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "        Transforms String review in dataframe into int encoded review.\n",
    "        \n",
    "    :param df: Dataframe that to transform\n",
    "    :param vocab_to_int: Word to int mapping as a dictionary\n",
    "    :param batch_size: Batchsize for the training\n",
    "    :return: Transformed Dataframe\n",
    "    \"\"\"\n",
    "    reviews_int = []\n",
    "    for review in df['review']:\n",
    "        r = []\n",
    "        for w in review.split():\n",
    "            if w in vocab_to_int:\n",
    "                r.append(vocab_to_int[w])\n",
    "            else:\n",
    "                r.append(vocab_to_int[UNKNOWN_WORD_TOKEN])\n",
    "        reviews_int.append(r)\n",
    "        \n",
    "    features = pad_features(reviews_int, 200).reshape(-1, 200)\n",
    "    targets = df['sentiment'].values.reshape(-1, 1)\n",
    "    \n",
    "    df = np_to_tf_dataset(features, targets)\n",
    "    return df.shuffle(10000).batch(batch_size)\n",
    "\n",
    "\n",
    "def pad_features(reviews_int, seq_length):\n",
    "    \"\"\"\n",
    "        Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "        \n",
    "    :param reviews_int: Reviews as array of ints\n",
    "    :param seq_length: Length of Sequence for the review\n",
    "    :return: 0 padded review\n",
    "    \"\"\"\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype=int)\n",
    "\n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "\n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length - review_len))\n",
    "            new = zeroes + review\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "\n",
    "        features[i, :] = np.array(new)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def np_to_tf_dataset(np_X, np_y):\n",
    "    \"\"\"\n",
    "        Casts Numpy Arrays into Tensors.\n",
    "        \n",
    "    :param np_X: Features Array\n",
    "    :param np_y: Target Array\n",
    "    :return: Tensor containing features and targets\n",
    "    \"\"\"\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(np_X, tf.float32),\n",
    "            tf.cast(np_y, tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "train['review'] = train['review'].apply(lambda x: x.lower())\n",
    "train['review'] = train['review'].apply(lambda x: ''.join([c for c in x if c not in punctuation]))\n",
    "\n",
    "test['review'] = test['review'].apply(lambda x: x.lower())\n",
    "test['review'] = test['review'].apply(lambda x: ''.join([c for c in x if c not in punctuation]))\n",
    "   \n",
    "# Encoding \n",
    "all_text = ' '.join(train['review'])\n",
    "words = all_text.split()\n",
    "count_words = Counter(words)\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)\n",
    "    \n",
    "vocab_to_int = {w: i + 1 for i, (w, c) in enumerate(sorted_words)}\n",
    "vocab_to_int[UNKNOWN_WORD_TOKEN] = len(vocab_to_int)\n",
    "    \n",
    "train_ds = word_to_int_mapping(train, vocab_to_int, 32)\n",
    "test_ds = word_to_int_mapping(test, vocab_to_int, 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.SentimentLSTM object at 0x13603a9e8>\n"
     ]
    }
   ],
   "source": [
    "class SentimentLSTM(Model):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, lstm_layers):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim, input_length=200)\n",
    "        self.lstm1= LSTM(lstm_layers, dropout=0.2, activation='tanh', return_sequences=True)\n",
    "        self.lstm2 = LSTM(lstm_layers, dropout=0.2, activation='tanh')\n",
    "        self.dropout = Dropout(0.3)\n",
    "        self.dense = Dense(output_size, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Vocab_size + 1 due to the 0 padding.\n",
    "vocab_size = len(vocab_to_int) + 1 \n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "lstm_layers = 256\n",
    "model = SentimentLSTM(vocab_size, output_size, embedding_dim, lstm_layers)\n",
    "# TODO find way to make print statement show layers similar to pytorch.\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The article use a Loss FUnction called BCELoss which stands for Binary Cross Entropy. The BinaryCrossentropy is the Tensorflow Counterpart and will be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryAccuracy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.BinaryAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisits for tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the training and validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course finally training the model. This will train for quite some time. You can either reduce the neurons in the network or use a strong GPU to train the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))\n",
    "    \n",
    "    # Reset metrics every epoch\n",
    "    train_loss.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_accuracy.reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs/gradient_tape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO code small applet where people can test it out (flask template on github or here in notebook)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
