{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Sentiment Analysis LSTM Network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disclaimer\n",
    "I tried to rewrite the neural network described in the Towards Datascience article \"Sentiment Analysis using LSTM\" (http://app.n26.com) using the IMDB Review Dataset from http://ai.stanford.edu/~amaas/data/sentiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout \n",
    "\n",
    "import urllib.request\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First download the data from 'from http://ai.stanford.edu/~amaas/data/sentiment'. This takes some time to run, so please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset...\n",
      "Unpacking dataset...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print('Downloading dataset...')\n",
    "\n",
    "url = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "urllib.request.urlretrieve(url, 'data.tar.gz')\n",
    "\n",
    "print('Unpacking dataset...')\n",
    "\n",
    "with tarfile.open('data.tar.gz', \"r:gz\") as tar:\n",
    "    tar.extractall()\n",
    "    \n",
    "print('Done.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the dataframes by loading the data from the downloaded file and adding the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>For a movie that gets no respect there sure ar...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bizarre horror movie filled with famous faces ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A solid, if unremarkable film. Matthau, as Ein...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It's a strange feeling to sit alone in a theat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You probably all already know this by now, but...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  For a movie that gets no respect there sure ar...          1\n",
       "1  Bizarre horror movie filled with famous faces ...          1\n",
       "2  A solid, if unremarkable film. Matthau, as Ein...          1\n",
       "3  It's a strange feeling to sit alone in a theat...          1\n",
       "4  You probably all already know this by now, but...          1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_dataset(train_test):\n",
    "    \"\"\"\n",
    "        Creates a Dataframe with one review column containing the reviews as Strings and a \n",
    "        sentiment column of datatype int with 0 being negative and 1 being positive sentiment.\n",
    "        \n",
    "    :param train_test: Accepts either 'train' or 'test' to create the respective Dataframe. \n",
    "    :return: A Dataframe containing the reviews and the sentiment.\n",
    "    \"\"\"\n",
    "    pos = load_data_as_dataframe(train_test, 'pos')\n",
    "    neg = load_data_as_dataframe(train_test, 'neg')\n",
    "    \n",
    "    df = pd.DataFrame(pos)\n",
    "    df.append(neg, ignore_index=True)\n",
    "    \n",
    "    df['sentiment'] = pd.to_numeric(df['sentiment'])\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_data_as_dataframe(train_test, label):\n",
    "    \"\"\"\n",
    "        This function retrieves the data with a certain label from the directory specified.\n",
    "    \n",
    "    :param train_test: A string being either 'train' or 'test', specifying which directory should be targeted.\n",
    "    :param label: A string being either 'pos' or 'neg' specifying if the positive or the negative reviews should be assembled.\n",
    "    :return: A Dataframe with a review column (dtype=String) and a sentiment column (dtype=Int).\n",
    "    \"\"\"\n",
    "    sentiment = 1 if label == 'pos' else 0\n",
    "    reviews = gather_reviews_from_directory(train_test, label)\n",
    "    reviews = pd.DataFrame({'review': reviews})\n",
    "    reviews['sentiment'] = pd.Series(np.array([sentiment] * len(reviews)), index=reviews.index)\n",
    "    return reviews\n",
    "\n",
    "\n",
    "def gather_reviews_from_directory(train_test, label):\n",
    "    \"\"\"\n",
    "        This function gathers the reviews from the single files and returns them as a Pandas \n",
    "        Series Object.\n",
    "        \n",
    "    :param label: A string being either 'pos' or 'neg' specifying if the positive or the negative reviews should be assembled.\n",
    "    :return: None\n",
    "    \"\"\"\n",
    "    content = []\n",
    "    for filepath in os.listdir('aclImdb/' + train_test + '/' + label):\n",
    "        with open(('aclImdb/' + train_test + '/' + label + '/' + filepath), 'r') as file:\n",
    "            review = file.read()\n",
    "            content.append(review)\n",
    "            \n",
    "    return pd.Series(content)\n",
    "\n",
    "\n",
    "train = create_dataset('train')\n",
    "test = create_dataset('test')\n",
    "\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A major change to the code in the article is in how words that occur in the training but not in the test set are handled. In the article the mapping is collected from the entire dataset, here it is collected only from the training set. This makes an unknown word token necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNKNOWN_WORD_TOKEN = -1\n",
    "\n",
    "\n",
    "def word_to_int_mapping(df, vocab_to_int, batch_size):\n",
    "    \"\"\"\n",
    "        Transforms String review in dataframe into int encoded review.\n",
    "        \n",
    "    :param df: Dataframe that to transform\n",
    "    :param vocab_to_int: Word to int mapping as a dictionary\n",
    "    :param batch_size: Batchsize for the training\n",
    "    :return: Transformed Dataframe\n",
    "    \"\"\"\n",
    "    reviews_int = []\n",
    "    for review in df['review']:\n",
    "        r = []\n",
    "        for w in review.split():\n",
    "            if w in vocab_to_int:\n",
    "                r.append(vocab_to_int[w])\n",
    "            else:\n",
    "                r.append(vocab_to_int[UNKNOWN_WORD_TOKEN])\n",
    "        reviews_int.append(r)\n",
    "        \n",
    "    features = pad_features(reviews_int, 200).reshape(-1, 200)\n",
    "    targets = df['sentiment'].values.reshape(-1, 1)\n",
    "    \n",
    "    df = np_to_tf_dataset(features, targets)\n",
    "    return df.shuffle(10000).batch(batch_size)\n",
    "\n",
    "\n",
    "def pad_features(reviews_int, seq_length):\n",
    "    \"\"\"\n",
    "        Return features of review_ints, where each review is padded with 0's or truncated to the input seq_length.\n",
    "        \n",
    "    :param reviews_int: Reviews as array of ints\n",
    "    :param seq_length: Length of Sequence for the review\n",
    "    :return: 0 padded review\n",
    "    \"\"\"\n",
    "    features = np.zeros((len(reviews_int), seq_length), dtype=int)\n",
    "\n",
    "    for i, review in enumerate(reviews_int):\n",
    "        review_len = len(review)\n",
    "\n",
    "        if review_len <= seq_length:\n",
    "            zeroes = list(np.zeros(seq_length - review_len))\n",
    "            new = zeroes + review\n",
    "        elif review_len > seq_length:\n",
    "            new = review[0:seq_length]\n",
    "\n",
    "        features[i, :] = np.array(new)\n",
    "\n",
    "    return features\n",
    "\n",
    "\n",
    "def np_to_tf_dataset(np_X, np_y):\n",
    "    \"\"\"\n",
    "        Casts Numpy Arrays into Tensors.\n",
    "        \n",
    "    :param np_X: Features Array\n",
    "    :param np_y: Target Array\n",
    "    :return: Tensor containing features and targets\n",
    "    \"\"\"\n",
    "    return tf.data.Dataset.from_tensor_slices(\n",
    "        (\n",
    "            tf.cast(np_X, tf.float32),\n",
    "            tf.cast(np_y, tf.float32)\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "train['review'] = train['review'].apply(lambda x: x.lower())\n",
    "train['review'] = train['review'].apply(lambda x: ''.join([c for c in x if c not in punctuation]))\n",
    "\n",
    "test['review'] = test['review'].apply(lambda x: x.lower())\n",
    "test['review'] = test['review'].apply(lambda x: ''.join([c for c in x if c not in punctuation]))\n",
    "   \n",
    "# Encoding \n",
    "all_text = ' '.join(train['review'])\n",
    "words = all_text.split()\n",
    "count_words = Counter(words)\n",
    "total_words = len(words)\n",
    "sorted_words = count_words.most_common(total_words)\n",
    "    \n",
    "vocab_to_int = {w: i + 1 for i, (w, c) in enumerate(sorted_words)}\n",
    "vocab_to_int[UNKNOWN_WORD_TOKEN] = len(vocab_to_int)\n",
    "    \n",
    "train_ds = word_to_int_mapping(train, vocab_to_int, 32)\n",
    "test_ds = word_to_int_mapping(test, vocab_to_int, 32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.embeddings.Embedding at 0x13d36c588>,\n",
       " <tensorflow.python.keras.layers.recurrent.UnifiedLSTM at 0x13d36c550>,\n",
       " <tensorflow.python.keras.layers.recurrent.UnifiedLSTM at 0x13d36fe10>,\n",
       " <tensorflow.python.keras.layers.core.Dropout at 0x13d36f2b0>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x13d348e48>]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class SentimentLSTM(Model):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, lstm_layers):\n",
    "        super(SentimentLSTM, self).__init__()\n",
    "        self.embedding = Embedding(vocab_size, embedding_dim, input_length=200)\n",
    "        self.lstm1= LSTM(lstm_layers, dropout=0.2, activation='tanh', return_sequences=True)\n",
    "        self.lstm2 = LSTM(lstm_layers, dropout=0.2, activation='tanh')\n",
    "        self.dropout = Dropout(0.3)\n",
    "        self.dense = Dense(output_size, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.lstm1(x)\n",
    "        x = self.lstm2(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Vocab_size + 1 due to the 0 padding.\n",
    "vocab_size = len(vocab_to_int) + 1 \n",
    "output_size = 1\n",
    "embedding_dim = 400\n",
    "lstm_layers = 256\n",
    "model = SentimentLSTM(vocab_size, output_size, embedding_dim, lstm_layers)\n",
    "model.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The article use a Loss FUnction called BCELoss which stands for Binary Cross Entropy. The BinaryCrossentropy is the Tensorflow Counterpart and will be used here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.BinaryCrossentropy(name='train_accuracy')\n",
    "\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.BinaryCrossentropy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisits for tensorboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "test_log_dir = 'logs/gradient_tape/' + current_time + '/test'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "test_summary_writer = tf.summary.create_file_writer(test_log_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up the training and validation function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course finally training the model. This will train for quite some time. You can either reduce the neurons in the network or use a strong GPU to train the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_ds:\n",
    "        train_step(images, labels)\n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_accuracy.result(), step=epoch)\n",
    "\n",
    "    for test_images, test_labels in test_ds:\n",
    "        test_step(test_images, test_labels)\n",
    "    with test_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', test_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', test_accuracy.result(), step=epoch)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                        train_loss.result(),\n",
    "                        train_accuracy.result()*100,\n",
    "                        test_loss.result(),\n",
    "                        test_accuracy.result()*100))\n",
    "    \n",
    "    # Reset metrics every epoch\n",
    "    train_loss.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_accuracy.reset_states()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, dir='output'):\n",
    "    os.mkdir(dir)\n",
    "    try:\n",
    "        model.save_weights('weights.hd5')\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "    \n",
    "    model.save_weights('weights.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type 'tensorboard --logdir logs/gradient_tape' into your console in the SentimentAnalysisIMDB folder to view the tensorboard metrics. In this particular example they are not helpfull at all because the network pretty much immediately has top performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x13aefe748>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_model(dir)\n",
    "model = SentimentLSTM(vocab_size, output_size, embedding_dim, lstm_layers)\n",
    "model.load_weights(os.path(dir, 'weights.hd5'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_review(test_review):\n",
    "    test_review = test_review.lower()\n",
    "    # get rid of punctuation\n",
    "    test_text = ''.join([c for c in test_review if c not in punctuation])\n",
    "\n",
    "    # splitting by spaces\n",
    "    test_words = test_text.split()\n",
    "\n",
    "    # tokens\n",
    "    test_ints = []\n",
    "    for w in test_words:\n",
    "        if w in vocab_to_int:\n",
    "            test_ints.append(vocab_to_int[w])\n",
    "        else:\n",
    "            test_ints.append(vocab_to_int[UNKNOWN_WORD_TOKEN])\n",
    "    return test_ints\n",
    "\n",
    "\n",
    "def predict(net, test_review, sequence_length=200):\n",
    "    print(type(test_review))\n",
    "    # tokenize review\n",
    "    test_ints = tokenize_review(test_review)\n",
    "    \n",
    "    # pad tokenized sequence\n",
    "    seq_length=sequence_length\n",
    "    features = pad_features([test_ints], seq_length)\n",
    "    \n",
    "    # convert to tensor to pass into your model\n",
    "    feature_tensor = tf.cast(features, tf.float32)\n",
    "    \n",
    "    batch_size = 1\n",
    "        \n",
    "    # get the output from the model\n",
    "    pred = net(feature_tensor)\n",
    "    \n",
    "    # convert output probabilities to predicted class (0 or 1)\n",
    "    # printing output value, before rounding\n",
    "    return pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I loved it.\n",
      "<class 'str'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.49781293], dtype=float32)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "your_review = input()\n",
    "pred = predict(model, your_review)\n",
    "pred.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
